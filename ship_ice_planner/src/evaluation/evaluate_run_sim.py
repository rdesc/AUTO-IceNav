""" Evaluation script of simulation experiments generated by sim_exp.py """
import glob
import json
import os
from distutils.dir_util import copy_tree

import numpy as np
import pandas as pd
import seaborn as sns
import pickle5 as pickle

import matplotlib
from matplotlib import pyplot as plt
from matplotlib.ticker import PercentFormatter
from tqdm import tqdm
from scipy.stats import norm

from ship_ice_planner.src.evaluation.metrics import *
from ship_ice_planner.src.geometry.polygon import poly_area
from ship_ice_planner.src.primitives import Primitives
from ship_ice_planner.src.ship import Ship
from ship_ice_planner.src.utils.plot import Plot
from ship_ice_planner.src.utils.utils import DotDict


font = {'family' : 'normal',
        'size'   : 12}
matplotlib.rc('font', **font)
pd.set_option('display.max_columns', None)


# parameters
# need a forward slash at the end of string
ROOT_DIR = 'output/icra_sim_exp/'  # root directory containing all the experiments
SAVE_FIG = True                    # save snapshots of planning steps
SHOW = False                       # show planning snapshots


def copy_trials(new_dir, old_dir, planners):
    dirs = glob.glob(old_dir + '*/*')
    for d in tqdm(dirs):
        if not os.path.isdir(d):
            continue
        concentration = os.path.basename(d)
        trials_dir = os.listdir(d)
        for tr in trials_dir:
            if not os.path.isdir(os.path.join(old_dir, concentration, tr)):
                continue
            for p in planners:
                copy_tree(os.path.join(old_dir, concentration, tr, p), os.path.join(new_dir, concentration, tr, p))


def generate_plot(dir_path, save_fig, show, plot_cost_map=False, show_cllns=False):
    """
    Generate plot for each captured planning step
    TODO: add time step in title and show ke as a function of time as a 3rd/2nd subplot
    """
    print(dir_path)
    cfg = DotDict.load_from_file(os.path.join(dir_path, 'config.yaml'))
    path_files = glob.glob(os.path.join(dir_path, 'paths', '*.pkl'))
    path_files = sorted(path_files, key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))

    planner = cfg.planner

    pose_data = []
    past_path = []
    track_error = []
    prev_path = None
    obs_collide = None
    prev_obs = None

    # iterate over each planning step
    for idx, fp in enumerate(tqdm(path_files)):
        try:
            data = pickle.load(open(fp, 'rb'))
        except EOFError:
            print('EOFError!', fp)
            continue
        iteration = os.path.splitext(os.path.basename(fp))[0]

        raw = data['raw']
        costmap = np.zeros((cfg.costmap.m, cfg.costmap.n))  # dummy map

        if 'obstacles' not in raw:
            if prev_obs is None:
                continue
            raw_obstacles = prev_obs
        else:
            raw_obstacles = [{'vertices': v} for v in raw['obstacles']]
            prev_obs = raw_obstacles

        ship_state = raw['ship_state']

        if prev_path is not None:
            # approximate tracking error
            err, track_idx = tracking_error(ship_state, prev_path.T, get_idx=True)
            past_path.append(prev_path.T[track_idx])
            track_error.append(err)

        pose_data.append(ship_state)
        # vertices = Ship(scale=1, **cfg.ship).vertices
        vertices = cfg.sim.vertices

        if 'path' not in data and 'best_path' not in data:
            print('Failed run!', fp)
            plot = Plot(
                costmap, raw_obstacles,
                ship_vertices=vertices, ship_pos=np.asarray(pose_data).T,
                sim_figsize=None, y_axis_limit=cfg.costmap.m,
            )

        elif planner in ['skeleton', 'straight']:
            path = data['path']
            if np.shape(path)[1] == 0:
                print('EMPTY PATH!')
                continue

            prev_path = path

            if planner == 'skeleton':
                path[:2] /= cfg.costmap.scale
                prev_path = path

            plot = Plot(
                costmap, raw_obstacles, path,
                ship_vertices=vertices, ship_pos=np.asarray(pose_data).T, horizon=None,
                sim_figsize=None, y_axis_limit=cfg.costmap.m,
            )

        else:
            path = data['path']
            node_path = (data['node_path'][0], data['node_path'][1])
            nodes_expanded = data['expanded']
            vertices = np.asarray(vertices) * cfg.costmap.scale
            costmap = data['costmap'] if plot_cost_map else np.zeros_like(data['costmap'])
            pose_data_scaled = np.asarray(pose_data).T
            pose_data_scaled[:2] *= cfg.costmap.scale

            plot = Plot(
                costmap,
                [{'vertices': ob['vertices'] * cfg.costmap.scale} for ob in raw_obstacles], path,
                path_nodes=node_path, nodes_expanded=nodes_expanded,
                swath=data['swath'], horizon=cfg.a_star.horizon * cfg.costmap.scale,
                sim_figsize=None, scale=cfg.costmap.scale, ship_pos=pose_data_scaled, ship_vertices=vertices,
                y_axis_limit=cfg.costmap.m * cfg.costmap.scale,
                # turning_radius=cfg.prim.turning_radius * cfg.prim.scale
            )
            # plot.show_prims_from_nodes_edges(plot.map_ax, Primitives(**cfg.prim), data['node_path'].T, data['edge_seq'])

            path[:2] /= cfg.costmap.scale
            prev_path = path

        if past_path:
            scale = cfg.costmap.scale if planner == 'lattice' else 1
            plot.map_ax.plot(np.asarray(past_path).T[0] * scale,
                             np.asarray(past_path).T[1] * scale, 'r--', linewidth=1)
        plt.suptitle('Planning iteration {}'.format(iteration))

        if show_cllns:
            # TODO: do collision check between two convex polygons
            # plot collision and log them in a dataframe
            return NotImplementedError

        if idx == len(path_files) - 1:
            # save out tracking error
            df = pd.DataFrame(track_error, columns=['e_x', 'e_y', 'e_yaw'])
            df.to_csv(os.path.join(dir_path, 'approx_track_error.csv'))
            plt.suptitle('Planning iteration {}\nmean track error {:.2f}, {:.2f}, {:.2f}'
                         .format(iteration, df['e_x'].mean(), df['e_y'].mean(), df['e_yaw'].mean()))

        if show:
            plt.show()
        if save_fig:
            plot.save(os.path.join(dir_path, 'plots'), iteration, im_format='png')
        plt.close(plot.map_fig)


def compute_path_metrics(path, scale=1):
    return {
        'path_length': path_length(path / scale),
        'curvature': curvature(path / scale),
        'path_smoothness': path_smoothness(path / scale),
        'velocity_smoothness': velocity_smoothness(path / scale),
    }


def plot_work_vs_time(planner_paths):
    """
    Plot work as a function of time for each planner
    """
    fig, ax = plt.subplots(figsize=(8, 6))
    for p in planner_paths:
        planner = os.path.basename(p)
        with open(os.path.join(p, 'state_history.txt'), 'r') as f:
            logs = f.readlines()

        # convert to df
        data = pd.DataFrame([json.loads(item) for item in logs])

        # add to plot
        ax.plot(data['time'], data['total_impulse'] / 1e5, label=planner)

    ax.legend()
    ax.set_xlabel('time (s)')
    ax.set_ylabel(r'total kinetic energy lost $\times 10^5$')
    fig.tight_layout()
    fig.savefig(os.path.join(os.path.split(planner_paths[0])[0], 'ke_vs_time.png'))
    plt.close(fig)


def process_trial(dir_path, save_fig=SAVE_FIG, show=SHOW):
    """
    Process one experiment trial across all planners
    """
    results = []
    collision_stats = {}
    planners = [p for p in os.listdir(dir_path) if os.path.isdir(os.path.join(dir_path, p))]
    baseline_work = None  # get the total work from straight planner for later use
    for p in planners:
        fp = os.path.join(dir_path, p)

        # run plotting code if necessary
        if save_fig or show:
            generate_plot(fp, save_fig, show, plot_cost_map=True, show_cllns=False)

        # get relevant metrics from controller logs
        with open(os.path.join(fp, 'state_history.txt'), 'r') as f:
            c_logs = f.readlines()

        # convert to df
        # metrics we want are: work, time, and mean track error
        c_logs = pd.DataFrame([json.loads(item) for item in c_logs])
        c_metrics = {
            **c_logs.iloc[-1][['time', 'total_work', 'total_ke', 'total_impulse']].to_dict(),
            **c_logs.mean()[['e_yaw']].add_suffix('_avg').to_dict(),  # rad
            'e_xy_avg': ((c_logs['e_x'] ** 2 + c_logs['e_y'] ** 2) ** 0.5).mean(),  # metres
        }

        if p == 'straight':
            baseline_work = c_metrics['total_work']
            baseline_ke = c_metrics['total_ke']
            baseline_impulse = c_metrics['total_impulse']

        # get the path
        path = c_logs[['x', 'y']].to_numpy()

        # get relevant metrics from planner logs
        log_file = os.path.join(fp, 'metrics.txt')
        if os.path.isfile(log_file):
            with open(log_file, 'r') as f:
                p_logs = f.readlines()
            # convert to df
            # metrics we want are: total iterations, mean compute time
            p_logs = pd.DataFrame([json.loads(item) for item in p_logs])
            p_metrics = {
                'total_iterations': len(p_logs),
                **p_logs.mean()[['compute_time']].add_suffix('_avg').to_dict()
            }

        else:
            assert p == 'straight'
            p_metrics = {'total_iterations': 1, 'compute_time_avg': 0}

        # add to results!
        results.append({
            **c_metrics,
            **p_metrics,
            **compute_path_metrics(path),
        })

        # collect collision data for single trial and planner
        raw_clln_data = pickle.load(open(os.path.join(fp, 'collision', 'raw.pk'), 'rb'))
        collision_stats[p] = {'clln_obs_mass': [poly_area(obs) for obs in raw_clln_data['clln_obs']],
                              'all_obs_mass': [poly_area(obs) for obs in raw_clln_data['all_obs']],
                              'contact_pts': raw_clln_data['contact_pts'].T[1].tolist()}

    # get the straight planner index
    for p, res in zip(planners, results):
        # compute the ratio of work to work done in straight planner
        res['total_work_ratio'] = res['total_work'] / baseline_work
        res['total_ke_ratio'] = res['total_ke'] / baseline_ke
        res['total_impulse_ratio'] = res['total_impulse'] / baseline_impulse

    df = pd.DataFrame(results, index=planners)
    df = df.reindex(sorted(df.columns), axis=1)
    df.to_csv(os.path.join(dir_path, 'results.csv'))

    # make work vs time line plots
    plot_work_vs_time([os.path.join(dir_path, p) for p in planners])

    # save collision data
    pickle.dump(collision_stats, open(os.path.join(dir_path, 'collisions.pk'), 'wb'))


def process_concentration(dir_path):
    """
    Process a set of trials for a particular ice field concentration
    """
    frames = []
    clln_stats = []
    trials = sorted([p for p in os.listdir(dir_path) if os.path.isdir(os.path.join(dir_path, p))])
    for t in trials:
        df = pd.read_csv(os.path.join(dir_path, t, 'results.csv'), index_col=0)

        # add an explict column for planner
        df['planner'] = df.index

        # add a column for the trial and concentration
        df['trial'] = t
        df['concentration'] = os.path.basename(dir_path)

        frames.append(df)

        # collect collision data across whole trial
        clln_stats.append(pickle.load(open(os.path.join(dir_path, t, 'collisions.pk'), 'rb')))

    df = pd.concat(frames, ignore_index=True)

    # store averaged results
    df_avg = pd.concat([df.groupby(['planner']).mean(),
                        df.groupby(['planner']).std().add_suffix('_sd')], axis=1)

    # compute the geometric mean for total work ratio https://dl.acm.org/doi/pdf/10.1145/5666.5673
    num_trials = len(df) / len(df.groupby(['planner']))
    df_cols0 = df[['total_work_ratio', 'planner']]
    geo_mean = df_cols0.groupby(['planner']).prod() ** (1 / num_trials)
    df_avg['total_work_ratio_g'] = geo_mean

    # geometric standard deviation
    # https://en.wikipedia.org/wiki/Geometric_standard_deviation
    df_avg['total_work_ratio_gsd'] = [
        np.exp(((np.log(df_cols0[df_cols0['planner'] == p]['total_work_ratio'] / geo_mean.loc[p].item()) ** 2).sum()
                / num_trials) ** 0.5)
        for p in geo_mean.index
    ]

    num_trials = len(df) / len(df.groupby(['planner']))
    df_cols1 = df[['total_impulse_ratio', 'planner']]
    geo_mean = df_cols1.groupby(['planner']).prod() ** (1 / num_trials)
    df_avg['total_impulse_ratio_g'] = geo_mean

    # geometric standard deviation
    # https://en.wikipedia.org/wiki/Geometric_standard_deviation
    df_avg['total_impulse_ratio_gsd'] = [
        np.exp(((np.log(df_cols1[df_cols1['planner'] == p]['total_impulse_ratio'] / geo_mean.loc[p].item()) ** 2).sum()
                / num_trials) ** 0.5)
        for p in geo_mean.index
    ]

    df_avg = df_avg.reindex(sorted(df_avg.columns), axis=1)
    df_avg.to_csv(os.path.join(dir_path, 'results.csv'))

    # aggregate collision data
    clln_dict = {}
    for planner in clln_stats[0].keys():
        dicts = []
        for d in clln_stats:
            dicts.append(d[planner])
        agg_dict = {}
        for k in d[planner]:
            agg_dict[k] = np.concatenate([item[k] for item in dicts])
        clln_dict[planner] = agg_dict

    # plot histograms
    fig0, ax0 = plt.subplots(figsize=(6, 8))
    for planner in clln_stats[0].keys():
        hist_data1 = clln_dict[planner]['contact_pts']
        fig1, ax1 = plt.subplots(figsize=(12, 9))
        ax1.hist(hist_data1, bins=20, weights=np.ones(len(hist_data1)) / len(hist_data1))
        ax1.set_ylim([0, 0.35])
        ax1.set_xlabel('distance from centerline')
        ax1.set_ylabel('portion of impacts')
        ax1.yaxis.set_major_formatter(PercentFormatter(1.))
        ax1.set_title('Planner {}\n % impacts vs. lateral distance from centerline'.format(planner))
        fig1.savefig(os.path.join(dir_path, 'contact_hist_' + planner + '.png'))

        hist_data0 = sorted(clln_dict[planner]['clln_obs_mass'])
        ax0.plot(hist_data0, norm.pdf(hist_data0, loc=np.mean(hist_data0), scale=np.std(hist_data0)),
                 label=f'{planner} - {str(len(hist_data0))}')
        # print('mean', np.mean(data0), 'planner', planner, 'ice', dir_path)
        # to read
        # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html#scipy.stats.norm

        plt.close('all')

    ax0.legend(loc='upper right')
    ax0.set_xlabel('ice mass')
    ax0.set_ylabel('probability density')
    ax0.set_title('Ice concentration ' + os.path.basename(dir_path))
    fig0.tight_layout()
    fig0.savefig(os.path.join(dir_path, 'mass_hist.png'))

    pickle.dump(clln_dict, open(os.path.join(dir_path, 'collisions.pk'), 'wb'))

    return df


def violin_plot(fp):  # FIXME: remove hardcode
    ice_col = 'ice concentration'
    work_col = 'total kinetic energy loss'
    print_ice_cols = [0.2, 0.3, 0.4, 0.5]
    print_planners = ['lattice', 'skeleton']
    print_cols = [work_col]

    # rename columns for purposes of plotting
    df = pd.read_csv(fp, index_col=0)
    df.rename(columns={'concentration': ice_col, 'total_impulse_ratio': work_col}, inplace=True)

    print(df[df[work_col] > 1][['planner', work_col, ice_col, 'trial']])

    # print mean and median for each ice concentration
    for i in print_ice_cols:
        print('\nIce concentration', i)
        for p in print_planners:
            print('\n\tplanner', p)
            print('mean', df[(df[ice_col] == i) & (df['planner'] == p)][print_cols].mean())
            print('median', df[(df[ice_col] == i) & (df['planner'] == p)][print_cols].median())
            print('max', df[(df[ice_col] == i) & (df['planner'] == p)][print_cols].max())
            print('min', df[(df[ice_col] == i) & (df['planner'] == p)][print_cols].min())

    df = df[(df['planner'] == 'skeleton') | (df['planner'] == 'lattice')]
    df = df.sort_values(by=[ice_col])
    ax = sns.boxplot(x=ice_col, y=work_col, hue='planner',
                     data=df, palette='muted', showfliers=False)
    plt.legend(loc='upper left')
    plt.tight_layout()
    plt.grid()
    # plt.show()
    plt.savefig(os.path.join(os.path.dirname(fp), 'violin_total_ke.png'), dpi=300)


def process_experiments(root_dir):
    # iterate over trials first
    dirs = glob.glob(root_dir + '*/*')
    for d in tqdm(dirs):
        if not os.path.isdir(d):
            continue
        process_trial(d)

    # iterate over the different ice field concentrations
    frames = []
    dirs = [os.path.join(root_dir, d) for d in os.listdir(root_dir)]
    for d in tqdm(dirs):
        if not os.path.isdir(d):
            continue
        frames.append(process_concentration(d))

    # store entire df
    fp = os.path.join(root_dir, 'raw.csv')
    pd.concat(frames, ignore_index=True).to_csv(fp)

    # make violin plot comparing skeleton and lattice
    violin_plot(fp)


if __name__ == '__main__':
    process_experiments(ROOT_DIR)
